{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac6615a2",
   "metadata": {},
   "source": [
    "# DSCI 525 - Web and Cloud Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27734fa",
   "metadata": {},
   "source": [
    "***Milestone 4:*** In this milestone, you will deploy the machine learning model you trained in milestone 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7b54bb",
   "metadata": {},
   "source": [
    "Milestone 4 checklist :\n",
    "\n",
    "- [x] Use an EC2 instance.\n",
    "- [x] Develop your API here in this notebook.\n",
    "- [x] Copy it to ```app.py``` file in EC2 instance.\n",
    "- [x] Run your API for other consumers and test among your colleagues.\n",
    "- [x] Summarize your journey."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b139cc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import all the packages that you need\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e7098e",
   "metadata": {},
   "source": [
    "## 1. Develop your API\n",
    "\n",
    "rubric={mechanics:45}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba1d8d1",
   "metadata": {},
   "source": [
    "You probably got how to set up primary URL endpoints from the ```sampleproject.ipynb notebook``` and have them process and return some data. Here we are going to create a new endpoint that accepts a POST request of the features required to run the machine learning model that you trained and saved in last milestone (i.e., a user will post the predictions of the 25 climate model rainfall predictions, i.e., features,  needed to predict with your machine learning model). Your code should then process this data, use your model to make a prediction, and return that prediction to the user. To get you started with all this, I've given you a template which you should fill out to set up this functionality:\n",
    "\n",
    "***NOTE:*** You won't be able to test the flask module (or the API you make here) unless you go through steps in ```2. Deploy your API```. However, here you can make sure that you develop all your functions and inputs properly.\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# 1. Load your model here\n",
    "model = joblib.load(...)\n",
    "\n",
    "# 2. Define a prediction function\n",
    "def return_prediction(...):\n",
    "\n",
    "    # format input_data here so that you can pass it to model.predict()\n",
    "\n",
    "    return model.predict(...)\n",
    "\n",
    "# 3. Set up home page using basic html\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    # feel free to customize this if you like\n",
    "    return \"\"\"\n",
    "    <h1>Welcome to our rain prediction service</h1>\n",
    "    To use this service, make a JSON post request to the /predict url with 5 climate model outputs.\n",
    "    \"\"\"\n",
    "\n",
    "# 4. define a new route which will accept POST requests and return model predictions\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def rainfall_prediction():\n",
    "    content = request.json  # this extracts the JSON content we sent\n",
    "    prediction = return_prediction(...)\n",
    "    results = {...}  # return whatever data you wish, it can be just the prediction\n",
    "                     # or it can be the prediction plus the input data, it's up to you\n",
    "    return jsonify(results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9666604",
   "metadata": {},
   "source": [
    "**Solution for Task 1:**\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# 1. Load your model here\n",
    "model = joblib.load(\"model.joblib\")\n",
    "# 2. Define a prediction function\n",
    "def return_prediction(input_data):\n",
    "    # format input_data here so that you can pass it to model.predict()\n",
    "    data = input_data[\"data\"]\n",
    "    pred = np.array(data).reshape(1, 25)\n",
    "    return model.predict(pred)\n",
    "\n",
    "\n",
    "# 3. Set up home page using basic html\n",
    "@app.route(\"/\")\n",
    "def index():\n",
    "    # feel free to customize this if you like\n",
    "    return \"\"\"\n",
    "    <h1>Welcome to our rain prediction service</h1>\n",
    "    To use this service, make a JSON post request to the /predict url with 5 climate model outputs.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "# 4. define a new route which will accept POST requests and return model predictions\n",
    "@app.route(\"/predict\", methods=[\"POST\"])\n",
    "def rainfall_prediction():\n",
    "    content = request.json  # this extracts the JSON content we sent\n",
    "    prediction = return_prediction(content)\n",
    "    results = {\n",
    "        \"Prediction\": prediction.tolist()\n",
    "    }  # return whatever data you wish, it can be just the prediction\n",
    "    # or it can be the prediction plus the input data, it's up to you\n",
    "    return jsonify(results)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65062d76",
   "metadata": {},
   "source": [
    "## 2. Deploy your API\n",
    "\n",
    "rubric={mechanics:40}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba67760",
   "metadata": {},
   "source": [
    "Once your API (app.py) is working we're ready to deploy it! For this, do the following:\n",
    "\n",
    "1. SSH into your EC2 instance from milestone2. There are no issues if you want to spin another EC2 instance; if you plan to do so, make sure you terminate any other running instances.\n",
    "2. Make a file `app.py` file in your instance and copy what you developed above in there. \n",
    "\n",
    "    2.1 You can use the linux editor using ```vi```. More details on vi Editor [here](https://www.guru99.com/the-vi-editor.html). I do recommend doing it this way and knowing some basics like ```:wq,:q!,dd``` will help.\n",
    "    \n",
    "    2.2 Or else you can make a file in your laptop called app.py and copy it over to your EC2 instance using ```scp```. Eg: ```scp -r -i \"ggeorgeAD.pem\" ~/Desktop/worker.py  ubuntu@ec2-xxx.ca-central-1.compute.amazonaws.com:~/```\n",
    "\n",
    "3. Download your model from s3 to your EC2 instance.\n",
    "4. Presumably you already have `pip` or `conda` installed on your instance from your previous milestone. You should use one of those package managers to install the dependencies of your API, like `flask`, `joblib`, `sklearn`, etc.\n",
    "\n",
    "    4.1. You have installed it in your TLJH using [Installing pip packages](https://tljh.jupyter.org/en/latest/howto/env/user-environment.html#installing-pip-packages). if you want to make it available to users outside of jupyterHub (which you want to in this case as we are logging into EC2 instance as user ```ubuntu``` by giving ```ssh -i privatekey ubuntu@<host_name>```) you can follow these [instructions](https://tljh.jupyter.org/en/latest/howto/env/user-environment.html#accessing-user-environment-outside-jupyterhub).\n",
    "    \n",
    "    4.2. Alternatively you can install the required packages inside your terminal.\n",
    "        - Install conda:\n",
    "            wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "            bash Miniconda3-latest-Linux-x86_64.sh\n",
    "        - Install packages (there might be others): \n",
    "            conda install flask scikit-learn joblib\n",
    "\n",
    "5. Now you're ready to start your service, go ahead and run `flask run --host=0.0.0.0 --port=8080`. This will make your service available at your EC2 instance's IP address on port 8080. Please make sure that you run this from where ```app.py``` and ```model.joblib``` resides.\n",
    "6. You can now access your service by typing your EC2 instances public IPv4 address appened with `:8080` into a browswer, so something like `http://<your_EC2_ip>:8080`.\n",
    "7. You should use `curl` to send a post request to your service to make sure it's working as expected.\n",
    ">EG: curl -X POST http://your_EC2_ip:8080/predict -d '{\"data\":[1,2,3,4,53,11,22,37,41,53,11,24,31,44,53,11,22,35,42,53,12,23,31,42,53]}' -H \"Content-Type: application/json\"\n",
    "\n",
    "8. Now, what happens if you exit your connection with the EC2 instance? Can you still reach your service?\n",
    "9. There are several options we could use to help us persist our server even after we exit our shell session. We'll be using `screen`. `screen` will allow us to create a separate session within which we can run `flask` and which won't shut down when we exit the main shell session. Read [this](https://linuxize.com/post/how-to-use-linux-screen/) to learn more on ```screen```.\n",
    "10. Now, create a new `screen` session (think of this as a new, separate shell), using: `screen -S myapi`. If you want to list already created sessions do ```screen -list```. If you want to get into an existing ```screen -x myapi```.\n",
    "11. Within that session, start up your flask app. You can then exit the session by pressing `Ctrl + A then press D`. Here you are detaching the session, once you log back into EC2 instance you can attach it using ```screen -x myapi```.\n",
    "12. Feel free to exit your connection with the EC2 instance now and try accessing your service again with `curl`. You should find that the service has now persisted!\n",
    "13. ***CONGRATULATIONS!!!*** You have successfully got to the end of our milestones. Move to Task 3 and submit it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2c3686",
   "metadata": {},
   "source": [
    "**Screenshot for Task 2:**\n",
    "\n",
    "![task2](https://raw.githubusercontent.com/UBC-MDS/525_group16/main/notebooks/milestone4/Screenshots/task3_milestone4.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb970d7e",
   "metadata": {},
   "source": [
    "## 3. Summarize your journey from Milestone 1 to Milestone 4\n",
    "rubric={mechanics:10}\n",
    ">There is no format or structure on how you write this. (also, no minimum number of words).  It's your choice on how well you describe it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df6ca2e3",
   "metadata": {},
   "source": [
    "**Solution for Task 3:**\n",
    "\n",
    "> **Milestone 1**:  In this milestone, the overall goal here was to make us aware of how difficult it was to work with large datasets on our local machine. Downloading the data from the web and extracting it and, combining it was particularly frustrating for team members who didn't have more than enough memory on their computer. Sometimes it even crashed the kernel! We found that performing simple operations on the combined data, using pandas, was time and memory intensive, so we explored using alternatives such as DASK, and loading just the columns we were interested in. The latter two methods seemed to reduce the time and memory requirements. Finally, we had to transfer the dataframe (i.e file) from python to R, and after exploring multiple approaches such as arrow exchange, feather file exchange, and parquet file exchange, we found out that the Parquet file exchange was the best for the task at hand. This milestone gave us insight into the need for web and cloud computing services when working with big data. \n",
    ">\n",
    ">**Milestone 2**: This milestone involved setting up the cloud infrastructure needed to complete the analysis of the climate modelling data. The Amazon Web Services (AWS) platform was used for both storage and analysis of the data. An EC2 instance (type t2.xlarge, 30 GB storage) was created for processing the data. The \"Ubuntu Server 18.04 LTS (HVM), SSD Volume Type 64-bit (x86)\" AMI was loaded into the instance along with The Littlest JupyterHub (TLJH). A TLJH document was created to download the required parquet data for the analysis from figshare.com to the EC2 instance.\n",
    "An S3 bucket was also created as part of Milestone 2 for data storage. The parquet data which had been downloaded to the EC2 instance was moved to the S3 bucket along with the ‘observed_daily_rainfall_SYD.csv’ file generated in Milestone 1 on a local computer.\n",
    ">\n",
    ">**Milestone 3**: The first part of this milestone was to set up our EC2 instance to create a EMR cluster with Spark and JupyterHub. Once the EMR cluster was started we needed to set up an SSH connection. With this, we were able to SSH into the Amazon EMR Master Node and set up our JupyterHub that was AWS managed in cluster. After installing required packages we then moved into developing a Machine Learning model for the climate data. The next task involved using our JupyterHub created from the previous milestone (TLJH in EC2) to download the climate data from the S3 bucket to create a baseline RandomForestRegressor ensemble model of all climate model predictions. Finally, in the JupyterHub in the EMR cluster we created in this milestone, we found the best hyperparameter settings for our RandomForestRegressor ensemble model using Spark's MLlib. The best settings were then exported back into the initial JupyterHub and our final model was saved to a .joblib file.\n",
    ">\n",
    ">**Milestone 4**: This milestone includes development and deployment of the machine learning model we created in the last milestone as a REST-API, in order for it to be accessed and utilised by other people and software. We used Flask as a simple web framework to help us build a service that receive a POST request of the features required to run the machine learning model we trained, process them and send the prediction back to the user. First, we set up a home page for our service. Then, we defined a function that extracting JSON content from the user’s request, passing it to our model and making predictions. After that, we returned the prediction back to the user. We deployed our API with our model in the EC2 instance and persisted our server after detaching the shell session by creating a separate session using `screen`. In the end, we successfully having our machine learning model deployed in the cloud for others to use. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b67ebabf",
   "metadata": {},
   "source": [
    "## 4. Submission instructions\n",
    "rubric={mechanics:5}\n",
    "\n",
    "In the textbox provided on Canvas please put a link where TAs can find the following-\n",
    "- [x] This notebook with solution to ```1 & 3```\n",
    "- [x] Screenshot from \n",
    "    - [x] Output after trying curl. Here is a [sample](https://github.ubc.ca/MDS-2020-21/DSCI_525_web-cloud-comp_students/blob/master/Milestones/milestone4/images/curl_deploy_sample.png). This is just an example; your input/output doesn't have to look like this, you can design the way you like. But at a minimum, it should show your prediction value."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:573]",
   "language": "python",
   "name": "conda-env-573-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
