{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adolescent-miniature",
   "metadata": {},
   "source": [
    "# Milestone 1\n",
    "## DSCI 525 Web and Cloud Computing\n",
    "## Group 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "frequent-finder",
   "metadata": {},
   "source": [
    "This notebook downloads various observed and simulated rainfall data sets from New South Wales, Australia over the period of 1889 - 2014.  The data are then combined and basic exporatory data analyses are conducted using both Python and R programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "starting-paste",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "alive-webmaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-flooring",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "The following code chunk downloads the data used in the subsequent analyses.  The data are downloaded from 'figshare.com'.  The file 'data.zip' is saved to a local directory called 'data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "convenient-analysis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 164.12 MiB, increment: 1.61 MiB\n",
      "CPU times: user 2.87 s, sys: 3.79 s, total: 6.66 s\n",
      "Wall time: 31.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Print out time and memory taken for downloading data\n",
    "\n",
    "# This code is adapted from DSCI 525 lecture demonstration notebook (Gittu George, 2021,\n",
    "# https://github.ubc.ca/MDS-2020-21/DSCI_525_web-cloud-comp_students/blob/master/Lectures/Lecture_1_2.ipynb)\n",
    "url = f\"https://api.figshare.com/v2/articles/14096681\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"data/\"\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "files = data[\"files\"]\n",
    "\n",
    "for file in files:\n",
    "    if file[\"name\"] in \"data.zip\":\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acknowledged-edward",
   "metadata": {},
   "source": [
    "After it has been downloaded locally, 'data.zip' is extracted and stored in the 'data' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aggregate-practitioner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 164.68 MiB, increment: 0.04 MiB\n",
      "CPU times: user 16 s, sys: 2.16 s, total: 18.1 s\n",
      "Wall time: 18.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Print out time and memory taken to extract data\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), \"r\") as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "weekly-length",
   "metadata": {},
   "source": [
    "So annoying to load all csvs into ram, combine, then resave.  Would be much easier if we could stitch the files together directly without loading them into RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "usual-specification",
   "metadata": {},
   "source": [
    "# Combining Data\n",
    "The following code chunk combines all of the unzipped rainfall data .csv files into a single file called 'combined_data.csv'.  This process is accomplished by creating a pandas dataframe called `full_df`, then one by one loading each .csv file and concatenating it with `full_df`.  This requires that all of the .csv files be read into a pandas dataframe variable and held in RAM at once.  In this case, this requires that almost 7 GB of data be held in RAM and manipulated.  Some computers will not be able to perform this data combining operation because they do not have sufficient RAM.  Even for systems which have sufficient RAM, performing simple operations (such as concatenation) on on a variable of this size are time consuming.  To demonstrate this, below the code chunk, we have included screen shots of the time and memory usage for the execution of this data combining operation.  To summarize, the time taken to complete this operation on each system are listed below (along with some general hardware specifications):\n",
    "1.  Wall time: 7min 9s; Peak memory: 6891.53 MiB\n",
    "  - Processor: i7-10510U (4 cores, up to 4.90 GHz)\n",
    "  - RAM: 16 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "previous-welsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 6950.41 MiB, increment: 6785.72 MiB\n",
      "CPU times: user 6min 25s, sys: 15.7 s, total: 6min 41s\n",
      "Wall time: 6min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Print out time and memory taken to merge and save csv files\n",
    "\n",
    "file_names = os.listdir(output_directory)\n",
    "file_names = [file for file in file_names if file[-4:] == \".csv\"]\n",
    "\n",
    "\n",
    "cols = [\"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\"]\n",
    "full_df = pd.DataFrame(columns=[\"model\"] + cols)\n",
    "full_df.index.rename(\"time\", inplace=True)\n",
    "\n",
    "for file in file_names:\n",
    "    model_name = re.search(\"^.*(?=_daily)\", file).group(0)\n",
    "    full_df = pd.concat(\n",
    "        [\n",
    "            full_df,\n",
    "            pd.read_csv(output_directory + file, index_col=0).assign(model=model_name),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "full_df.to_csv(output_directory + \"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-tribe",
   "metadata": {},
   "source": [
    "1. Processor: i7-10510U (4 cores, up to 4.90 GHz); RAM: 16 GB\n",
    "\n",
    "![](../img/i7-10510_16GB-SP.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "external-frame",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525]",
   "language": "python",
   "name": "conda-env-525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
