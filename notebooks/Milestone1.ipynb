{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "important-inquiry",
   "metadata": {},
   "source": [
    "# Milestone 1\n",
    "## DSCI 525 Web and Cloud Computing\n",
    "## Group 16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "portable-footwear",
   "metadata": {},
   "source": [
    "This notebook downloads various observed and simulated rainfall data sets from New South Wales, Australia over the period of 1889 - 2014.  The data are then combined and basic exporatory data analyses are conducted using both Python and R programming languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "favorite-seeker",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import rpy2.rinterface\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.feather as feather\n",
    "# import rpy2.rinterface\n",
    "# import rpy2_arrow.pyarrow_rarrow as pyra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "white-agenda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fifty-balance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%R\n",
    "# library(dplyr)\n",
    "# library(arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-gateway",
   "metadata": {},
   "source": [
    "# Data Download\n",
    "The following code chunk downloads the data used in the subsequent analyses.  The data are downloaded from 'figshare.com'.  The file 'data.zip' is saved to a local directory called 'data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "social-railway",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 109.27 MiB, increment: 3.66 MiB\n",
      "Wall time: 35min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Print out time and memory taken for downloading data\n",
    "\n",
    "# This code is adapted from DSCI 525 lecture demonstration notebook (Gittu George, 2021,\n",
    "# https://github.ubc.ca/MDS-2020-21/DSCI_525_web-cloud-comp_students/blob/master/Lectures/Lecture_1_2.ipynb)\n",
    "url = f\"https://api.figshare.com/v2/articles/14096681\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data/\"\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "files = data[\"files\"]\n",
    "\n",
    "for file in files:\n",
    "    if file[\"name\"] in \"data.zip\":\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pretty-dover",
   "metadata": {},
   "source": [
    "After it has been downloaded locally, 'data.zip' is extracted and stored in the 'data' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "adequate-commons",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 114.94 MiB, increment: 5.75 MiB\n",
      "Wall time: 16.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Print out time and memory taken to extract data\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), \"r\") as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "similar-actor",
   "metadata": {},
   "source": [
    "So annoying to load all csvs into ram, combine, then resave.  Would be much easier if we could stitch the files together directly without loading them into RAM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-insight",
   "metadata": {},
   "source": [
    "# Combining Data\n",
    "The following code chunk combines all of the unzipped rainfall data .csv files into a single file called 'combined_data.csv'.  This process is accomplished by creating a pandas dataframe called `full_df`, then one by one loading each .csv file and concatenating it with `full_df`.  This requires that all of the .csv files be read into a pandas dataframe variable and held in RAM at once.  In this case, this requires that almost 7 GB of data be held in RAM and manipulated.  Some computers will not be able to perform this data combining operation because they do not have sufficient RAM.  Even for systems which have sufficient RAM, performing simple operations (such as concatenation) on on a variable of this size are time consuming.  To demonstrate this, below the code chunk, we have included screen shots of the time and memory usage for the execution of this data combining operation.  To summarize, the time taken to complete this operation on each system are listed below (along with some general hardware specifications):\n",
    "1. Wall time: 7min 9s; Peak memory: 6891.53 MiB\n",
    "    - Processor: i7-10510U (4 cores, up to 4.90 GHz)\n",
    "    - RAM: 16 GB\n",
    "2. Wall time: 9min 46s; Peak memory: 3097.45 MiB\n",
    "    - Processor: i5\n",
    "    - RAM: 8 GB\n",
    "3. Wall time: 6min 5s; Peak memory: 7265.16 MiB\n",
    "    - Processor: i7-8700K (6 cores, up to 3.70 GHz)\n",
    "    - RAM: 16 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "limited-dimension",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 7362.87 MiB, increment: 7253.09 MiB\n",
      "Wall time: 6min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Print out time and memory taken to merge and save csv files\n",
    "\n",
    "file_names = os.listdir(output_directory)\n",
    "file_names = [file for file in file_names if file[-4:] == \".csv\"]\n",
    "\n",
    "cols = [\"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\"]\n",
    "full_df = pd.DataFrame(columns=[\"model\"] + cols)\n",
    "full_df.index.rename(\"time\", inplace=True)\n",
    "\n",
    "for file in file_names:\n",
    "    result = re.search(\"^.*(?=_daily)\", file)\n",
    "    if result:\n",
    "        model_name = result.group(0)\n",
    "        full_df = pd.concat(\n",
    "            [\n",
    "                full_df,\n",
    "                pd.read_csv(output_directory + file, index_col=0).assign(\n",
    "                    model=model_name\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "full_df.to_csv(output_directory + \"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "colonial-roller",
   "metadata": {},
   "source": [
    "1. Processor: i7-10510U (4 cores, up to 4.90 GHz); RAM: 16 GB\n",
    "\n",
    "![](../img/i7-10510_16GB-SP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "informal-cowboy",
   "metadata": {},
   "source": [
    "2. Processor: 2.3 GHz Quad-Core Intel Core i5; RAM: 8GB\n",
    "\n",
    "![](../img/i5_8GB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hired-mountain",
   "metadata": {},
   "source": [
    "3. Processor: i7-8700K (6 cores, up to 3.70 GHz); RAM: 16 GB\n",
    "\n",
    "![](../img/i7-8700K_16GB_CZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-jacob",
   "metadata": {},
   "source": [
    "## Task 5. Load the combined CSV to memory and perform a simple EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-china",
   "metadata": {},
   "source": [
    "### 1. Investigate at least 2 approaches and perform a simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "indian-validity",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1889-01-01 12:00:00</th>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>3.293256e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-02 12:00:00</th>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-03 12:00:00</th>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-04 12:00:00</th>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-05 12:00:00</th>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>1.047658e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  lat_min  lat_max  lon_min  lon_max  \\\n",
       "time                                                                  \n",
       "1889-01-01 12:00:00  ACCESS-CM2   -36.25    -35.0  140.625    142.5   \n",
       "1889-01-02 12:00:00  ACCESS-CM2   -36.25    -35.0  140.625    142.5   \n",
       "1889-01-03 12:00:00  ACCESS-CM2   -36.25    -35.0  140.625    142.5   \n",
       "1889-01-04 12:00:00  ACCESS-CM2   -36.25    -35.0  140.625    142.5   \n",
       "1889-01-05 12:00:00  ACCESS-CM2   -36.25    -35.0  140.625    142.5   \n",
       "\n",
       "                     rain (mm/day)  \n",
       "time                                \n",
       "1889-01-01 12:00:00   3.293256e-13  \n",
       "1889-01-02 12:00:00   0.000000e+00  \n",
       "1889-01-03 12:00:00   0.000000e+00  \n",
       "1889-01-04 12:00:00   0.000000e+00  \n",
       "1889-01-05 12:00:00   1.047658e-02  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fluid-prefix",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 62513863 entries, 1889-01-01 12:00:00 to 2014-12-31 12:00:00\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   model          object \n",
      " 1   lat_min        float64\n",
      " 2   lat_max        float64\n",
      " 3   lon_min        float64\n",
      " 4   lon_max        float64\n",
      " 5   rain (mm/day)  float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 3.3+ GB\n"
     ]
    }
   ],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "identified-prospect",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model             object\n",
       "lat_min          float64\n",
       "lat_max          float64\n",
       "lon_min          float64\n",
       "lon_max          float64\n",
       "rain (mm/day)    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southeast-grass",
   "metadata": {},
   "source": [
    "#### Method 1: Loading in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "limited-mother",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "AWI-ESM-1-1-LR       966420\n",
      "BCC-CSM2-MR         3035340\n",
      "BCC-ESM1             551880\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CanESM5              551880\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "FGOALS-f3-L         3219300\n",
      "FGOALS-g3           1287720\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "MIROC6              2070900\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-HR       5154240\n",
      "MPI-ESM1-2-LR        966420\n",
      "MRI-ESM2-0          3037320\n",
      "NESM3                966420\n",
      "NorESM2-LM           919800\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "TaiESM1             3541230\n",
      "observed              46020\n",
      "dtype: int32\n",
      "peak memory: 5708.41 MiB, increment: 2153.03 MiB\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "import dask.dataframe as dd\n",
    "\n",
    "### Code adapted from DSCI 525 Lecture ipynb notebook (Gittu George, 2021)\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"../data/combined_data.csv\", chunksize=10_000_000):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "composite-trial",
   "metadata": {},
   "source": [
    "#### Method 2: Using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "finite-velvet",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "NorESM2-LM           919800\n",
      "CanESM5              551880\n",
      "BCC-ESM1             551880\n",
      "observed              46020\n",
      "Name: model, dtype: int64\n",
      "peak memory: 6171.05 MiB, increment: 2402.82 MiB\n",
      "Wall time: 32.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "### Code adapted from DSCI 525 Lecture ipynb notebook (Gittu George, 2021)\n",
    "\n",
    "dask_df = dd.read_csv(\"../data/combined_data.csv\")\n",
    "print(dask_df[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "powered-music",
   "metadata": {},
   "source": [
    "#### Method 3: Loading just columns what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "african-bleeding",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "CMCC-ESM2           3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "NorESM2-MM          3541230\n",
      "TaiESM1             3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "MRI-ESM2-0          3037320\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM5-0           1609650\n",
      "INM-CM4-8           1609650\n",
      "FGOALS-g3           1287720\n",
      "KIOST-ESM           1287720\n",
      "AWI-ESM-1-1-LR       966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "observed              46020\n",
      "Name: model, dtype: int64\n",
      "peak memory: 4733.75 MiB, increment: 957.22 MiB\n",
      "Wall time: 37.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# The only column we want is the model column\n",
    "model_df = pd.read_csv(\"../data/combined_data.csv\", usecols=[\"model\"])\n",
    "print(model_df[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-fifteen",
   "metadata": {},
   "source": [
    "### 2. Observations discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vulnerable-karma",
   "metadata": {},
   "source": [
    "- Loading just the column we want seems to have the shortest CPU times (user 30.9 s, sys: 2.3 s, total: 33.2 s) and wall time (33.9 s). \n",
    "\n",
    "- Loading the combined data using Dask has a shorter wall time (40.6 s) than loading in chunks, however, it has longer CPU times (user 1min 24s, sys: 18.6 s, total: 1min 43s) than loading in chunks (user 59.6 s, sys: 7.12 s, total: 1min 6s).\n",
    "\n",
    "- Loading just the column we want has the minimum peak memory and increment used (1166.77 MiB and 780.00 MiB), whilst loading in chunks has the maximum peak memory and increment (1873.48 MiB, increment: 1458.30 MiB). \n",
    "\n",
    "- It is also worth noting that the memory usage from full_df.info() was memory usage: 3.3+ GB. Thus, using these methods to load the data all saved us considerable memory space. \n",
    "\n",
    "- In conclusion, loading just the column we want gives us the optimum time and space savings. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ahead-worcester",
   "metadata": {},
   "source": [
    "## Task 6. Perform a simple EDA in R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "played-cause",
   "metadata": {},
   "source": [
    "### 1. Store data in different format\n",
    "\n",
    "Here we will write the data in 2 more different formats to compare the running time and ocuppied storage between different formats. All formats of data in this section including:\n",
    "- csv format\n",
    "- feather format\n",
    "- parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "metallic-muscle",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 24.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = ds.dataset(\"../data/combined_data.csv\", format=\"csv\")\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stupid-object",
   "metadata": {},
   "source": [
    "**Feather format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "abandoned-watch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2.22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feather.write_feather(table, \"../data/example.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-empty",
   "metadata": {},
   "source": [
    "**Parquet format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "forbidden-queen",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 10.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pq.write_table(table, \"../data/example.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "conscious-hundred",
   "metadata": {},
   "source": [
    "**Check the size of data in all different formats**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "sitting-partner",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.7G\t../data/combined_data.csv\n",
      "1.1G\t../data/example.feather\n",
      "542M\t../data/example.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh ../data/combined_data.csv\n",
    "du -sh ../data/example.feather\n",
    "du -sh ../data/example.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dental-jackson",
   "metadata": {},
   "source": [
    "**Discussion 1:**\n",
    "\n",
    "- The parquet file format of data takes the least storage, while feather file is twice as big as the parquest file. The csv file is the largest, which we may not want to store on local machines\n",
    "- We can find out storing data into feather format is less time consuming by comparing to the running time of wtiting data into parquet format\n",
    "- The reason for storing parquest file with more time but less storage is because the data needs more layers of encoding and compressing during the saving process. If we are limited by the storage capability, we may need to pick the parquest as data file format. If we are more concerned about time consuming, storing data as feather format will be more suitable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "surface-collect",
   "metadata": {},
   "source": [
    "### 2. Transfer the dataframe from python to R and perform EDA\n",
    "\n",
    "Here we will experiment 3 exchange approaches to transfer the loaded dataset from python to R and perform EDA. In the end, we will pick one appropriate approach over others. All exchange approaches in this section including:\n",
    "- Arrow exchange\n",
    "- feather file exchange\n",
    "- parquet file exchange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-essay",
   "metadata": {},
   "source": [
    "**Arrow exchange and EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "human-guatemala",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "r_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-allowance",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R -i r_table\n",
    "start_time <- Sys.time()\n",
    "head_df <- head(r_table)\n",
    "glimpse_df <- glimpse(r_table)\n",
    "model_count <- r_table %>% collect() %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(class(r_table))\n",
    "print(head_df)\n",
    "print(glimpse_df)\n",
    "print(model_count)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assigned-poetry",
   "metadata": {},
   "source": [
    "**Feather file exchange and EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "committed-interpretation",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"../data/example.feather\")\n",
    "head_df <- head(r_table)\n",
    "glimpse_df <- glimpse(r_table)\n",
    "model_count <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(class(r_table))\n",
    "print(head_df)\n",
    "print(glimpse_df)\n",
    "print(model_count)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "related-transparency",
   "metadata": {},
   "source": [
    "**Parquet file exchange and EDA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%R\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_parquet(\"../data/example.parquet\")\n",
    "head_df <- head(r_table)\n",
    "glimpse_df <- glimpse(r_table)\n",
    "model_count <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(class(r_table))\n",
    "print(head_df)\n",
    "print(glimpse_df)\n",
    "print(model_count)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seven-audio",
   "metadata": {},
   "source": [
    "**Discussion 2:**\n",
    "\n",
    "- Although the running time for arrow exchange and EDA seems to be the shortest, we need to count in the time of loading the arrow dataframe as well\n",
    "- The total time consumption for arrow exhange and EDA, feather file loading and EDA, and parquet file loading and EDA is roughly the same\n",
    "- The functions used for arrow exchange is still in its baby state. Some functions are limited and need further development\n",
    "- Exchanging data by writing it to parquet from python and then reading in R is a mature way to deal with large size of data, and it is widely used within industry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "defensive-hopkins",
   "metadata": {},
   "source": [
    "### 3. Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sought-watson",
   "metadata": {},
   "source": [
    "We will pick parquet file format to transfer data from python to R. The reasons are listed as follows:\n",
    "\n",
    "- The Arrow memory format is an unified way to represent memory for efficient analytic operations. It saves the time and storage for serialization and deserialization. Parquet is a columnar based file format which can write arrow to disk and deal with big data efficiently\n",
    "- From \"Discussion 1\" above, when compared to csv file format, parquet can save the data in a file with far less size, and speed up the process largely when munipulating the data\n",
    "- Parquet file format can save data with least storage when comparing to csv and feather file format. Although it takes more time to write than feather as it features more layers of encoding and compression, when dealing with data size as in this excercise, it is not too much. Besides, with the least saving size, it means parquet file format will be the fastest when wrangling the data\n",
    "- From \"Discussion 2\" above, we can see that the total time consumption for arrow exhange and EDA, feather file loading and EDA, and parquet file loading and EDA is roughly the same, but parquet is approved to be the one with least time comsumption. When the data size is large enough, the small storage size and little time comsuption can be really important\n",
    "- Although feather is another good option to write data, as the speed is really fast, it is newly developed these years. The package is not mature enough and some functions can be unstable still\n",
    "\n",
    "In conclusion, when dealing with big data, and we want to storage the data for long-term, we will pick parquet file format, as it saves storage and is very efficient when analyzing big data due to its columnar based format. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baking-tribute",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsci525]",
   "language": "python",
   "name": "conda-env-dsci525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
