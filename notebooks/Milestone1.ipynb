{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "favorite-north",
   "metadata": {},
   "source": [
    "# Milestone 1\n",
    "## DSCI 525 Web and Cloud Computing\n",
    "## Group 16\n",
    "### Authors: Jared Splinter, Steffen Pentelow, Chen Zhao, Ifeanyi Anene"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fourth-worthy",
   "metadata": {},
   "source": [
    "This notebook downloads various observed and simulated rainfall data sets from New South Wales, Australia over the period of 1889 - 2014.  The data are then combined and basic exporatory data analyses are conducted using both Python and R programming languages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "naughty-connecticut",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "rapid-competition",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import zipfile\n",
    "import requests\n",
    "from urllib.request import urlretrieve\n",
    "import json\n",
    "import rpy2.rinterface\n",
    "import dask.dataframe as dd\n",
    "import pandas as pd\n",
    "from memory_profiler import memory_usage\n",
    "import pyarrow.dataset as ds\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.feather as feather\n",
    "import rpy2.rinterface\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "hybrid-merchandise",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jared\\miniconda3\\envs\\525\\lib\\site-packages\\rpy2\\robjects\\packages.py:366: UserWarning: The symbol 'quartz' is not in this R namespace/package.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "placed-burning",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: 'dplyr'\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from 'package:stats':\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from 'package:base':\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(dplyr)\n",
    "library(arrow)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "challenging-louisville",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "The following code chunk downloads the data used in the subsequent analyses.  The data are downloaded from 'figshare.com'.  The file 'data.zip' is saved to a local directory called 'data'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "polar-lithuania",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 292.42 MiB, increment: 3.66 MiB\n",
      "Wall time: 1min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Print out time and memory taken for downloading data\n",
    "\n",
    "# This code is adapted from DSCI 525 lecture demonstration notebook (Gittu George, 2021,\n",
    "# https://github.ubc.ca/MDS-2020-21/DSCI_525_web-cloud-comp_students/blob/master/Lectures/Lecture_1_2.ipynb)\n",
    "url = f\"https://api.figshare.com/v2/articles/14096681\"\n",
    "headers = {\"Content-Type\": \"application/json\"}\n",
    "output_directory = \"../data/\"\n",
    "\n",
    "response = requests.request(\"GET\", url, headers=headers)\n",
    "data = json.loads(response.text)\n",
    "files = data[\"files\"]\n",
    "\n",
    "for file in files:\n",
    "    if file[\"name\"] in \"data.zip\":\n",
    "        os.makedirs(output_directory, exist_ok=True)\n",
    "        urlretrieve(file[\"download_url\"], output_directory + file[\"name\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "typical-theme",
   "metadata": {},
   "source": [
    "After it has been downloaded locally, 'data.zip' is extracted and stored in the 'data' directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "marked-remark",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 297.81 MiB, increment: 5.44 MiB\n",
      "Wall time: 37.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Print out time and memory taken to extract data\n",
    "\n",
    "with zipfile.ZipFile(os.path.join(output_directory, \"data.zip\"), \"r\") as f:\n",
    "    f.extractall(output_directory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "suspected-hometown",
   "metadata": {},
   "source": [
    "### Discussion: \n",
    "\n",
    "Loading each .csv file into memory, concatenating it with the other .csv files, then writing back to disc is computationally and memory intensive. To load each .csv file, it must be deserialized (a process that takes some time with large files), then stored in RAM while being manipulated. Assuming a computer has sufficient RAM to hold all of the files, they can be concatenated together (e.g., using Pandas or Numpy), but then must be serialized again to be saved as a large .csv file. It would be better to be able to concatenate the files directly on disk or at least have them saved in a format where they could be read and written directly to and from disk without serialization/deserialization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-employer",
   "metadata": {},
   "source": [
    "## Combining Data\n",
    "The following code chunk combines all of the unzipped rainfall data .csv files into a single file called 'combined_data.csv'.  This process is accomplished by creating a pandas dataframe called `full_df`, then one by one loading each .csv file and concatenating it with `full_df`.  This requires that all of the .csv files be read into a pandas dataframe variable and held in RAM at once.  In this case, this requires that almost 7 GB of data be held in RAM and manipulated.  Some computers will not be able to perform this data combining operation because they do not have sufficient RAM.  Even for systems which have sufficient RAM, performing simple operations (such as concatenation) on on a variable of this size are time consuming.  To demonstrate this, below the code chunk, we have included screen shots of the time and memory usage for the execution of this data combining operation.  To summarize, the time taken to complete this operation on each system are listed below (along with some general hardware specifications):\n",
    "1. Wall time: 7min 9s; Peak memory: 6891.53 MiB\n",
    "    - Processor: i7-10510U (4 cores, up to 4.90 GHz)\n",
    "    - RAM: 16 GB\n",
    "2. Wall time: 9min 46s; Peak memory: 3097.45 MiB\n",
    "    - Processor: i5\n",
    "    - RAM: 8 GB\n",
    "3. Wall time: 6min 5s; Peak memory: 7265.16 MiB\n",
    "    - Processor: i7-8700K (6 cores, up to 3.70 GHz)\n",
    "    - RAM: 16 GB\n",
    "4. Wall time: 23min 43s; Peak memory: 2541.36 MiB\n",
    "    - Current Computer\n",
    "    - Processor i5-5300U (4 cores)\n",
    "    - RAM: 8 GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "complimentary-declaration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2541.36 MiB, increment: 2249.50 MiB\n",
      "Wall time: 23min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "# Print out time and memory taken to merge and save csv files\n",
    "\n",
    "file_names = os.listdir(output_directory)\n",
    "file_names = [file for file in file_names if file[-4:] == \".csv\"]\n",
    "\n",
    "cols = [\"lat_min\", \"lat_max\", \"lon_min\", \"lon_max\", \"rain (mm/day)\"]\n",
    "full_df = pd.DataFrame(columns=[\"model\"] + cols)\n",
    "full_df.index.rename(\"time\", inplace=True)\n",
    "\n",
    "for file in file_names:\n",
    "    result = re.search(\"^.*(?=_daily)\", file)\n",
    "    if result:\n",
    "        model_name = result.group(0)\n",
    "        full_df = pd.concat(\n",
    "            [\n",
    "                full_df,\n",
    "                pd.read_csv(output_directory + file, index_col=0).assign(\n",
    "                    model=model_name\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "full_df.to_csv(output_directory + \"combined_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accepting-patio",
   "metadata": {},
   "source": [
    "> **NOTE:** This notebook was last run with a i5-5300U CPU @ 2.30 GHz with 4 CPUS and 8GB of RAM and thus the run times in this notebook are worse. The runs for the above code cell on other machines are shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "considerable-cemetery",
   "metadata": {},
   "source": [
    "1. Processor: i7-10510U (4 cores, up to 4.90 GHz); RAM: 16 GB\n",
    "\n",
    "![](../img/i7-10510_16GB-SP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "impressed-lambda",
   "metadata": {},
   "source": [
    "2. Processor: 2.3 GHz Quad-Core Intel Core i5; RAM: 8GB\n",
    "\n",
    "![](../img/i5_8GB.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-showcase",
   "metadata": {},
   "source": [
    "3. Processor: i7-8700K (6 cores, up to 3.70 GHz); RAM: 16 GB\n",
    "\n",
    "![](../img/i7-8700K_16GB_CZ.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "favorite-migration",
   "metadata": {},
   "source": [
    "## Task 5. Load the combined CSV to memory and perform a simple EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dried-charm",
   "metadata": {},
   "source": [
    "### 1. Investigate at least 2 approaches and perform a simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "jewish-awareness",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>time</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1889-01-01 12:00:00</th>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>3.293256e-13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-02 12:00:00</th>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-03 12:00:00</th>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-04 12:00:00</th>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1889-01-05 12:00:00</th>\n",
       "      <td>ACCESS-CM2</td>\n",
       "      <td>-36.25</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>140.625</td>\n",
       "      <td>142.5</td>\n",
       "      <td>1.047658e-02</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          model  lat_min  lat_max  lon_min  lon_max  \\\n",
       "time                                                                  \n",
       "1889-01-01 12:00:00  ACCESS-CM2   -36.25    -35.0  140.625    142.5   \n",
       "1889-01-02 12:00:00  ACCESS-CM2   -36.25    -35.0  140.625    142.5   \n",
       "1889-01-03 12:00:00  ACCESS-CM2   -36.25    -35.0  140.625    142.5   \n",
       "1889-01-04 12:00:00  ACCESS-CM2   -36.25    -35.0  140.625    142.5   \n",
       "1889-01-05 12:00:00  ACCESS-CM2   -36.25    -35.0  140.625    142.5   \n",
       "\n",
       "                     rain (mm/day)  \n",
       "time                                \n",
       "1889-01-01 12:00:00   3.293256e-13  \n",
       "1889-01-02 12:00:00   0.000000e+00  \n",
       "1889-01-03 12:00:00   0.000000e+00  \n",
       "1889-01-04 12:00:00   0.000000e+00  \n",
       "1889-01-05 12:00:00   1.047658e-02  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "proved-latex",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 62513863 entries, 1889-01-01 12:00:00 to 2014-12-31 12:00:00\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Dtype  \n",
      "---  ------         -----  \n",
      " 0   model          object \n",
      " 1   lat_min        float64\n",
      " 2   lat_max        float64\n",
      " 3   lon_min        float64\n",
      " 4   lon_max        float64\n",
      " 5   rain (mm/day)  float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 3.3+ GB\n"
     ]
    }
   ],
   "source": [
    "full_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "declared-bidder",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model             object\n",
       "lat_min          float64\n",
       "lat_max          float64\n",
       "lon_min          float64\n",
       "lon_max          float64\n",
       "rain (mm/day)    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-miniature",
   "metadata": {},
   "source": [
    "#### Method 1: Loading in Chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "supposed-yesterday",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "AWI-ESM-1-1-LR       966420\n",
      "BCC-CSM2-MR         3035340\n",
      "BCC-ESM1             551880\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "CanESM5              551880\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "FGOALS-f3-L         3219300\n",
      "FGOALS-g3           1287720\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "MIROC6              2070900\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-HR       5154240\n",
      "MPI-ESM1-2-LR        966420\n",
      "MRI-ESM2-0          3037320\n",
      "NESM3                966420\n",
      "NorESM2-LM           919800\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "TaiESM1             3541230\n",
      "observed              46020\n",
      "dtype: int32\n",
      "peak memory: 1660.71 MiB, increment: 1372.91 MiB\n",
      "Wall time: 3min 12s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "import dask.dataframe as dd\n",
    "\n",
    "### Code adapted from DSCI 525 Lecture ipynb notebook (Gittu George, 2021)\n",
    "counts = pd.Series(dtype=int)\n",
    "for chunk in pd.read_csv(\"../data/combined_data.csv\", chunksize=10_000_000):\n",
    "    counts = counts.add(chunk[\"model\"].value_counts(), fill_value=0)\n",
    "print(counts.astype(int))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sitting-interview",
   "metadata": {},
   "source": [
    "#### Method 2: Using Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "affecting-charleston",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "TaiESM1             3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-ESM2           3541230\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "KIOST-ESM           1287720\n",
      "FGOALS-g3           1287720\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NESM3                966420\n",
      "AWI-ESM-1-1-LR       966420\n",
      "NorESM2-LM           919800\n",
      "CanESM5              551880\n",
      "BCC-ESM1             551880\n",
      "observed              46020\n",
      "Name: model, dtype: int64\n",
      "peak memory: 1144.84 MiB, increment: 853.45 MiB\n",
      "Wall time: 1min 43s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "### Code adapted from DSCI 525 Lecture ipynb notebook (Gittu George, 2021)\n",
    "\n",
    "dask_df = dd.read_csv(\"../data/combined_data.csv\")\n",
    "print(dask_df[\"model\"].value_counts().compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stuck-river",
   "metadata": {},
   "source": [
    "#### Method 3: Loading just columns what we want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "amazing-collapse",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPI-ESM1-2-HR       5154240\n",
      "CMCC-CM2-SR5        3541230\n",
      "CMCC-CM2-HR4        3541230\n",
      "TaiESM1             3541230\n",
      "CMCC-ESM2           3541230\n",
      "NorESM2-MM          3541230\n",
      "SAM0-UNICON         3541153\n",
      "FGOALS-f3-L         3219300\n",
      "GFDL-CM4            3219300\n",
      "GFDL-ESM4           3219300\n",
      "EC-Earth3-Veg-LR    3037320\n",
      "MRI-ESM2-0          3037320\n",
      "BCC-CSM2-MR         3035340\n",
      "MIROC6              2070900\n",
      "ACCESS-CM2          1932840\n",
      "ACCESS-ESM1-5       1610700\n",
      "INM-CM4-8           1609650\n",
      "INM-CM5-0           1609650\n",
      "FGOALS-g3           1287720\n",
      "KIOST-ESM           1287720\n",
      "AWI-ESM-1-1-LR       966420\n",
      "NESM3                966420\n",
      "MPI-ESM-1-2-HAM      966420\n",
      "MPI-ESM1-2-LR        966420\n",
      "NorESM2-LM           919800\n",
      "BCC-ESM1             551880\n",
      "CanESM5              551880\n",
      "observed              46020\n",
      "Name: model, dtype: int64\n",
      "peak memory: 1023.29 MiB, increment: 719.91 MiB\n",
      "Wall time: 1min 46s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "# The only column we want is the model column\n",
    "model_df = pd.read_csv(\"../data/combined_data.csv\", usecols=[\"model\"])\n",
    "print(model_df[\"model\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-isaac",
   "metadata": {},
   "source": [
    "### 2. Observations discussion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demonstrated-agenda",
   "metadata": {},
   "source": [
    "- Loading just the column we want seems to have the shortest CPU times (user 30.9 s, sys: 2.3 s, total: 33.2 s) and wall time (33.9 s). \n",
    "    - **NOTE:** These numbers come from a previous run, for this run Dask had the shortest run time with wall time of 1min 43s, 3 seconds faster than loading in the column\n",
    "\n",
    "\n",
    "- Loading the combined data using Dask has a shorter wall time (40.6 s) than loading in chunks, however, it has longer CPU times (user 1min 24s, sys: 18.6 s, total: 1min 43s) than loading in chunks (user 59.6 s, sys: 7.12 s, total: 1min 6s).\n",
    "\n",
    "- Loading just the column we want has the minimum peak memory and increment used (1166.77 MiB and 780.00 MiB), whilst loading in chunks has the maximum peak memory and increment (1873.48 MiB, increment: 1458.30 MiB). \n",
    "\n",
    "- It is also worth noting that the memory usage from full_df.info() was memory usage: 3.3+ GB. Thus, using these methods to load the data all saved us considerable memory space. \n",
    "\n",
    "- In conclusion, loading just the column we want gives us the optimal time and space savings. \n",
    "    - **NOTE:** In the last run, Dask had a slightly faster wall time, however loading in the column we want is still better considering space. Furthermore, as the wall times between Dask and loading in column method, we can conclude that loading in the column may be preferred.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "demanding-summer",
   "metadata": {},
   "source": [
    "## Task 6. Perform a simple EDA in R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hawaiian-release",
   "metadata": {},
   "source": [
    "### 1. Store data in different format\n",
    "\n",
    "Here we will write the data in 2 more different formats to compare the running time and ocuppied storage between different formats. All formats of data in this section including:\n",
    "- csv format\n",
    "- feather format\n",
    "- parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "professional-aspect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 21s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dataset = ds.dataset(\"../data/combined_data.csv\", format=\"csv\")\n",
    "table = dataset.to_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "differential-portugal",
   "metadata": {},
   "source": [
    "#### Method 1: Feather Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "hybrid-consultancy",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 39.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "feather.write_feather(table, \"../data/example.feather\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optional-frame",
   "metadata": {},
   "source": [
    "#### Method 2: Parquet format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "working-devon",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "pq.write_table(table, \"../data/example.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-harmony",
   "metadata": {},
   "source": [
    "#### Check the size of data in all different formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "general-litigation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.7G\t../data/combined_data.csv\n",
      "1.1G\t../data/example.feather\n",
      "542M\t../data/example.parquet\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "du -sh ../data/combined_data.csv\n",
    "du -sh ../data/example.feather\n",
    "du -sh ../data/example.parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charged-contamination",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "\n",
    "- The parquet file format of data takes the least storage, while feather file is twice as big as the parquest file. The csv file is the largest, which we may not want to store on local machines\n",
    "\n",
    "- It takes less time to write the feather file than it takes to write the parquet file.\n",
    "\n",
    "- The parquet file takes more time to write but uses less memory when loaded. This is because the data needs more layers of encoding and compressing during the saving process. If we are limited by the storage capability, the parquet file may be ideal. However, if we are more concerned about time, storing data as feather format may be more suitable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virtual-archive",
   "metadata": {},
   "source": [
    "### 2. Transfer the Dataframe from Python to R and perform EDA\n",
    "\n",
    "Here we will experiment 3 exchange approaches to transfer the loaded dataset from python to R and perform EDA. In the end, we will pick one appropriate approach over others. All exchange approaches in this section including:\n",
    "- Arrow exchange\n",
    "- feather file exchange\n",
    "- parquet file exchange"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "available-apparel",
   "metadata": {},
   "source": [
    "#### Arrow exchange and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "centered-wallet",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5756\n",
      "rarrow.ChunkedArray: 0.08099365234375\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.08300495147705078\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.05200505256652832\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.04401063919067383\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.05300617218017578\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.07300305366516113\n",
      "5756\n",
      "rarrow.ChunkedArray: 0.06899619102478027\n",
      "Wall time: 1min 22s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "r_table = pyra.converter.py2rpy(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "composite-remove",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes 'Table', 'ArrowObject', 'R6' <Table>\n",
      "  Inherits from: <ArrowObject>\n",
      "  Public:\n",
      "    .:xp:.: externalptr\n",
      "    AddColumn: function (i, new_field, value) \n",
      "    cast: function (target_schema, safe = TRUE, options = cast_options(safe)) \n",
      "    clone: function (deep = FALSE) \n",
      "    column: function (i) \n",
      "    ColumnNames: function () \n",
      "    columns: active binding\n",
      "    Equals: function (other, check_metadata = FALSE, ...) \n",
      "    field: function (i) \n",
      "    Filter: function (i, keep_na = TRUE) \n",
      "    GetColumnByName: function (name) \n",
      "    initialize: function (xp) \n",
      "    invalidate: function () \n",
      "    metadata: active binding\n",
      "    num_columns: active binding\n",
      "    num_rows: active binding\n",
      "    pointer: function () \n",
      "    print: function (...) \n",
      "    RemoveColumn: function (i) \n",
      "    RenameColumns: function (value) \n",
      "    schema: active binding\n",
      "    SelectColumns: function (indices) \n",
      "    serialize: function (output_stream, ...) \n",
      "    set_pointer: function (xp) \n",
      "    SetColumn: function (i, new_field, value) \n",
      "    Slice: function (offset, length = NULL) \n",
      "    Take: function (i) \n",
      "    ToString: function () \n",
      "    Validate: function () \n",
      "    ValidateFull: function ()  \n",
      "[1] \"Table\"       \"ArrowObject\" \"R6\"         \n",
      "Table\n",
      "6 rows x 7 columns\n",
      "$time <timestamp[s]>\n",
      "$model <string>\n",
      "$lat_min <double>\n",
      "$lat_max <double>\n",
      "$lon_min <double>\n",
      "$lon_max <double>\n",
      "$rain (mm/day) <double>\n",
      "Table\n",
      "62513863 rows x 7 columns\n",
      "$time <timestamp[s]>\n",
      "$model <string>\n",
      "$lat_min <double>\n",
      "$lat_max <double>\n",
      "$lon_min <double>\n",
      "$lon_max <double>\n",
      "$rain (mm/day) <double>\n",
      "# A tibble: 28 x 2\n",
      "   model                  n\n",
      "   <chr>              <int>\n",
      " 1 ACCESS-CM2       1932840\n",
      " 2 ACCESS-ESM1-5    1610700\n",
      " 3 AWI-ESM-1-1-LR    966420\n",
      " 4 BCC-CSM2-MR      3035340\n",
      " 5 BCC-ESM1          551880\n",
      " 6 CanESM5           551880\n",
      " 7 CMCC-CM2-HR4     3541230\n",
      " 8 CMCC-CM2-SR5     3541230\n",
      " 9 CMCC-ESM2        3541230\n",
      "10 EC-Earth3-Veg-LR 3037320\n",
      "# ... with 18 more rows\n",
      "Time difference of 58.63663 secs\n",
      "Wall time: 1min 2s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R -i r_table\n",
    "start_time <- Sys.time()\n",
    "head_df <- head(r_table)\n",
    "glimpse_df <- glimpse(r_table)\n",
    "model_count <- r_table %>% collect() %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(class(r_table))\n",
    "print(head_df)\n",
    "print(glimpse_df)\n",
    "print(model_count)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-committee",
   "metadata": {},
   "source": [
    "#### Feather file exchange and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "selected-configuration",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 62,513,863\n",
      "Columns: 7\n",
      "$ time            <dttm> 1889-01-01 04:00:00, 1889-01-02 04:00:00, 1889-01-...\n",
      "$ model           <chr> \"ACCESS-CM2\", \"ACCESS-CM2\", \"ACCESS-CM2\", \"ACCESS-C...\n",
      "$ lat_min         <dbl> -36.25, -36.25, -36.25, -36.25, -36.25, -36.25, -36...\n",
      "$ lat_max         <dbl> -35, -35, -35, -35, -35, -35, -35, -35, -35, -35, -...\n",
      "$ lon_min         <dbl> 140.625, 140.625, 140.625, 140.625, 140.625, 140.62...\n",
      "$ lon_max         <dbl> 142.5, 142.5, 142.5, 142.5, 142.5, 142.5, 142.5, 14...\n",
      "$ `rain (mm/day)` <dbl> 3.293256e-13, 0.000000e+00, 0.000000e+00, 0.000000e...\n",
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n",
      "# A tibble: 6 x 7\n",
      "  time                model      lat_min lat_max lon_min lon_max `rain (mm/day)`\n",
      "  <dttm>              <chr>        <dbl>   <dbl>   <dbl>   <dbl>           <dbl>\n",
      "1 1889-01-01 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        3.29e-13\n",
      "2 1889-01-02 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        0.      \n",
      "3 1889-01-03 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        0.      \n",
      "4 1889-01-04 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        0.      \n",
      "5 1889-01-05 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        1.05e- 2\n",
      "6 1889-01-06 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        3.29e- 2\n",
      "# A tibble: 62,513,863 x 7\n",
      "   time                model     lat_min lat_max lon_min lon_max `rain (mm/day)`\n",
      "   <dttm>              <chr>       <dbl>   <dbl>   <dbl>   <dbl>           <dbl>\n",
      " 1 1889-01-01 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        3.29e-13\n",
      " 2 1889-01-02 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        0.      \n",
      " 3 1889-01-03 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        0.      \n",
      " 4 1889-01-04 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        0.      \n",
      " 5 1889-01-05 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        1.05e- 2\n",
      " 6 1889-01-06 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        3.29e- 2\n",
      " 7 1889-01-07 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        8.91e- 2\n",
      " 8 1889-01-08 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        3.16e- 2\n",
      " 9 1889-01-09 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        3.11e- 2\n",
      "10 1889-01-10 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        3.30e- 2\n",
      "# ... with 62,513,853 more rows\n",
      "# A tibble: 28 x 2\n",
      "   model                  n\n",
      "   <chr>              <int>\n",
      " 1 ACCESS-CM2       1932840\n",
      " 2 ACCESS-ESM1-5    1610700\n",
      " 3 AWI-ESM-1-1-LR    966420\n",
      " 4 BCC-CSM2-MR      3035340\n",
      " 5 BCC-ESM1          551880\n",
      " 6 CanESM5           551880\n",
      " 7 CMCC-CM2-HR4     3541230\n",
      " 8 CMCC-CM2-SR5     3541230\n",
      " 9 CMCC-ESM2        3541230\n",
      "10 EC-Earth3-Veg-LR 3037320\n",
      "# ... with 18 more rows\n",
      "Time difference of 2.096566 mins\n",
      "Wall time: 2min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_feather(\"../data/example.feather\")\n",
    "head_df <- head(r_table)\n",
    "glimpse_df <- glimpse(r_table)\n",
    "model_count <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(class(r_table))\n",
    "print(head_df)\n",
    "print(glimpse_df)\n",
    "print(model_count)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "simplified-warrior",
   "metadata": {},
   "source": [
    "#### Parquet file exchange and EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "unnecessary-message",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows: 62,513,863\n",
      "Columns: 7\n",
      "$ time            <dttm> 1889-01-01 04:00:00, 1889-01-02 04:00:00, 1889-01-...\n",
      "$ model           <chr> \"ACCESS-CM2\", \"ACCESS-CM2\", \"ACCESS-CM2\", \"ACCESS-C...\n",
      "$ lat_min         <dbl> -36.25, -36.25, -36.25, -36.25, -36.25, -36.25, -36...\n",
      "$ lat_max         <dbl> -35, -35, -35, -35, -35, -35, -35, -35, -35, -35, -...\n",
      "$ lon_min         <dbl> 140.625, 140.625, 140.625, 140.625, 140.625, 140.62...\n",
      "$ lon_max         <dbl> 142.5, 142.5, 142.5, 142.5, 142.5, 142.5, 142.5, 14...\n",
      "$ `rain (mm/day)` <dbl> 3.293256e-13, 0.000000e+00, 0.000000e+00, 0.000000e...\n",
      "[1] \"tbl_df\"     \"tbl\"        \"data.frame\"\n",
      "# A tibble: 6 x 7\n",
      "  time                model      lat_min lat_max lon_min lon_max `rain (mm/day)`\n",
      "  <dttm>              <chr>        <dbl>   <dbl>   <dbl>   <dbl>           <dbl>\n",
      "1 1889-01-01 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        3.29e-13\n",
      "2 1889-01-02 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        0.      \n",
      "3 1889-01-03 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        0.      \n",
      "4 1889-01-04 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        0.      \n",
      "5 1889-01-05 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        1.05e- 2\n",
      "6 1889-01-06 04:00:00 ACCESS-CM2   -36.2     -35    141.    142.        3.29e- 2\n",
      "# A tibble: 62,513,863 x 7\n",
      "   time                model     lat_min lat_max lon_min lon_max `rain (mm/day)`\n",
      "   <dttm>              <chr>       <dbl>   <dbl>   <dbl>   <dbl>           <dbl>\n",
      " 1 1889-01-01 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        3.29e-13\n",
      " 2 1889-01-02 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        0.      \n",
      " 3 1889-01-03 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        0.      \n",
      " 4 1889-01-04 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        0.      \n",
      " 5 1889-01-05 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        1.05e- 2\n",
      " 6 1889-01-06 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        3.29e- 2\n",
      " 7 1889-01-07 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        8.91e- 2\n",
      " 8 1889-01-08 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        3.16e- 2\n",
      " 9 1889-01-09 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        3.11e- 2\n",
      "10 1889-01-10 04:00:00 ACCESS-C~   -36.2     -35    141.    142.        3.30e- 2\n",
      "# ... with 62,513,853 more rows\n",
      "# A tibble: 28 x 2\n",
      "   model                  n\n",
      "   <chr>              <int>\n",
      " 1 ACCESS-CM2       1932840\n",
      " 2 ACCESS-ESM1-5    1610700\n",
      " 3 AWI-ESM-1-1-LR    966420\n",
      " 4 BCC-CSM2-MR      3035340\n",
      " 5 BCC-ESM1          551880\n",
      " 6 CanESM5           551880\n",
      " 7 CMCC-CM2-HR4     3541230\n",
      " 8 CMCC-CM2-SR5     3541230\n",
      " 9 CMCC-ESM2        3541230\n",
      "10 EC-Earth3-Veg-LR 3037320\n",
      "# ... with 18 more rows\n",
      "Time difference of 1.082522 mins\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "start_time <- Sys.time()\n",
    "r_table <- arrow::read_parquet(\"../data/example.parquet\")\n",
    "head_df <- head(r_table)\n",
    "glimpse_df <- glimpse(r_table)\n",
    "model_count <- r_table %>% count(model)\n",
    "end_time <- Sys.time()\n",
    "print(class(r_table))\n",
    "print(head_df)\n",
    "print(glimpse_df)\n",
    "print(model_count)\n",
    "print(end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "included-career",
   "metadata": {},
   "source": [
    "### Discussion:\n",
    "\n",
    "- Although the running time for arrow exchange and EDA seems to be the shortest, we need to count in the time of loading the arrow dataframe as well\n",
    "\n",
    "- The total time consumption for arrow exhange and EDA, feather file loading and EDA, and parquet file loading and EDA is roughly the same\n",
    "    - Considering the size of the data we are working with all 3 methods tested run quickly  \n",
    "\n",
    "\n",
    "- Arrow exchange and the functions associated are still in development. Some functions and applications are limited and may be unstable. Further development is likely needed.\n",
    "\n",
    "- Exchanging data by writing it to parquet from Python and then reading in R is an efficiently optimized way to deal with large size of data, and it is widely used within industry."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chief-fourth",
   "metadata": {},
   "source": [
    "### 3. Summary\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "twelve-amber",
   "metadata": {},
   "source": [
    "Based on our observations, we will pick parquet file format to transfer data from Python to R for the following reasons:\n",
    "\n",
    "1. The Arrow memory format is an unified way to represent memory for efficient analytic operations. It saves the time and storage for serialization and deserialization. Parquet is a columnar based file format which can write arrow to disk and deal with big data efficiently\n",
    "\n",
    "2. Earlier we concluded a parquet file can store the data using much less space and memory. This can speed up the process largely when working with the data.\n",
    "\n",
    "3. Although it takes more time to write a parquet file compared to a feather file, the parquet file features more layers of encoding and compression. When dealing with data size as in this excercise, it is not too much. However, as the parquet file takes the least amount of space and memory compared to other methods it is the fastest when wrangling data.\n",
    "\n",
    "4. Earlier we concluded that the total time consumption for arrow exhange and EDA, feather file loading and EDA, and parquet file loading and EDA is roughly the same considering the size of data we are working with. However, parquet still manages the be the quickest. When working with large data, these small changes could potentially be really important.\n",
    "\n",
    "5. Although feather is another good option to work with large data and is very fast, it has only recently been developed. The package is not mature enough and some functions may be unstable.\n",
    "\n",
    "In conclusion, when dealing with big data, and we want to storage the data for long-term, we will pick parquet file format, as it saves storage and is very efficient when analyzing big data due to its columnar based format. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "antique-treasure",
   "metadata": {},
   "source": [
    "# Final Thoughts and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spectacular-resistance",
   "metadata": {},
   "source": [
    "> The challenge of working with large data is apparent. Loading in such a large data set proved to be a challenge among every laptop; taking up incredible amounts of space, pushing our laptop CPU's to the limit, and causing long code run times. \n",
    ">\n",
    ">In particular, in one situation, while re-running this notebook the memory on my laptop was completely used up and the kernel crashed. I needed to delete every picture from my laptop to overcome the storage requirements. A few times a process would crash and progress would be lost. In another occassion, an error kept persisting in a code cell one for not enough memory allocation and one for a process being used elsewhere. After some searching online it became apparent that my laptop was not capable of handling everything at once and a restart was needed. There is not much more demoralizing than realizing you need to load in a massive data set again and sit through and wait for the code to run because there is not enough memory, or a windows error. Perhaps that is what I deserve for having a laptop that uses Windows. \n",
    ">\n",
    "> It also became clear throughout this milestone that with large data a simple csv file and traditional data wrangling methods become very long and ineffective. Fortunately, there are other methods such as Dask, Feather/Parquet files, and Arrow exchange that can make working with such a large data set much easier and vastly improve run times. However, the run times when using the most efficient methods explored here can still take some time and memory to run. \n",
    ">\n",
    "> Finally, the importance of a good computer increases when working with large data. Among the machines tested in our group, those with better processors and more RAM were able to run code much faster. If one is working with large data frequently it may be a good investment to get a powerful computer with lots of RAM."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsci525]",
   "language": "python",
   "name": "conda-env-dsci525-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
